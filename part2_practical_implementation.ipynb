{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Part 2: Practical Implementation (50%)\n", "## Task 1: Classical ML with Scikit-learn \u2014 Iris Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from sklearn.datasets import load_iris\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n", "\n", "# Load iris dataset\n", "iris = load_iris()\n", "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n", "y = pd.Series(iris.target)\n", "\n", "# Check for missing values\n", "print(\"Missing values in features:\\n\", X.isnull().sum())\n", "\n", "# Split data: 70% train, 30% test\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n", "\n", "# Initialize and train Decision Tree Classifier\n", "clf = DecisionTreeClassifier(random_state=42)\n", "clf.fit(X_train, y_train)\n", "\n", "# Predict on test data\n", "y_pred = clf.predict(X_test)\n", "\n", "# Evaluate performance\n", "accuracy = accuracy_score(y_test, y_pred)\n", "precision = precision_score(y_test, y_pred, average='macro')\n", "recall = recall_score(y_test, y_pred, average='macro')\n", "\n", "print(f\"Accuracy: {accuracy:.4f}\")\n", "print(f\"Precision: {precision:.4f}\")\n", "print(f\"Recall: {recall:.4f}\")\n", "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 2: Deep Learning with TensorFlow \u2014 MNIST Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "from tensorflow.keras import layers, models\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "# Load MNIST dataset\n", "mnist = tf.keras.datasets.mnist\n", "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "\n", "# Normalize pixel values\n", "X_train, X_test = X_train / 255.0, X_test / 255.0\n", "\n", "# Add channel dimension\n", "X_train = X_train[..., np.newaxis]\n", "X_test = X_test[..., np.newaxis]\n", "\n", "# Build CNN model\n", "model = models.Sequential([\n", "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n", "    layers.MaxPooling2D((2,2)),\n", "    layers.Conv2D(64, (3,3), activation='relu'),\n", "    layers.MaxPooling2D((2,2)),\n", "    layers.Flatten(),\n", "    layers.Dense(64, activation='relu'),\n", "    layers.Dense(10, activation='softmax')\n", "])\n", "\n", "# Compile model\n", "model.compile(optimizer='adam',\n", "              loss='sparse_categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "# Train model\n", "history = model.fit(X_train, y_train, epochs=5, validation_split=0.1)\n", "\n", "# Evaluate on test set\n", "test_loss, test_acc = model.evaluate(X_test, y_test)\n", "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n", "\n", "# Visualize predictions on 5 samples\n", "predictions = model.predict(X_test[:5])\n", "for i in range(5):\n", "    plt.imshow(X_test[i].reshape(28,28), cmap='gray')\n", "    plt.title(f\"Predicted: {np.argmax(predictions[i])}, Actual: {y_test[i]}\")\n", "    plt.axis('off')\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 3: NLP with spaCy \u2014 Named Entity Recognition and Sentiment Analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import spacy\n", "\n", "# Load spaCy English model\n", "nlp = spacy.load(\"en_core_web_sm\")\n", "\n", "# Example Amazon product reviews\n", "reviews = [\n", "    \"I love the Apple iPhone 12! The camera quality is amazing.\",\n", "    \"The Samsung Galaxy S21 battery life is disappointing.\",\n", "    \"Great sound on the Bose QuietComfort headphones, highly recommend.\",\n", "    \"Terrible experience with the Dell Inspiron laptop, very slow.\",\n", "    \"The Sony WH-1000XM4 headphones have excellent noise cancellation.\"\n", "]\n", "\n", "# Sentiment keyword lists\n", "positive_words = [\"love\", \"amazing\", \"great\", \"highly recommend\", \"excellent\"]\n", "negative_words = [\"disappointing\", \"terrible\", \"slow\"]\n", "\n", "for review in reviews:\n", "    doc = nlp(review)\n", "    print(f\"\\nReview: {review}\")\n", "    print(\"Entities:\")\n", "    for ent in doc.ents:\n", "        print(f\" - {ent.text} ({ent.label_})\")\n", "    \n", "    # Simple sentiment analysis\n", "    review_lower = review.lower()\n", "    sentiment = \"Neutral\"\n", "    if any(word in review_lower for word in positive_words):\n", "        sentiment = \"Positive\"\n", "    elif any(word in review_lower for word in negative_words):\n", "        sentiment = \"Negative\"\n", "    print(f\"Sentiment: {sentiment}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}